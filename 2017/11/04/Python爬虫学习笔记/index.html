<!DOCTYPE html>
<html lang=en>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="今天开始学习写爬虫吧~">
<meta name="keywords" content="Python,爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="Python爬虫学习笔记">
<meta property="og:url" content="http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/index.html">
<meta property="og:site_name" content="看多了会困">
<meta property="og:description" content="今天开始学习写爬虫吧~">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://hexopic.s3-website-ap-northeast-1.amazonaws.com/tujiagou_login.png">
<meta property="og:image" content="http://images2015.cnblogs.com/blog/953786/201606/953786-20160612162715136-1486217043.jpg">
<meta property="og:updated_time" content="2020-02-24T23:04:46.291Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Python爬虫学习笔记">
<meta name="twitter:description" content="今天开始学习写爬虫吧~">
<meta name="twitter:image" content="http://hexopic.s3-website-ap-northeast-1.amazonaws.com/tujiagou_login.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Python爬虫学习笔记</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2017/11/11/自动化运维之Puppet/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2017/10/29/drbd实现HA的MySQL集群/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&text=Python爬虫学习笔记"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&is_video=false&description=Python爬虫学习笔记"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python爬虫学习笔记&body=Check out this article: http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&name=Python爬虫学习笔记&description=&lt;p&gt;今天开始学习写爬虫吧~&lt;/p&gt;"><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Python对XML-JSON-HTML的解析"><span class="toc-number">1.</span> <span class="toc-text">Python对XML,JSON,HTML的解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#xml"><span class="toc-number">1.0.1.</span> <span class="toc-text">xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JSON"><span class="toc-number">1.0.2.</span> <span class="toc-text">JSON</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HTML"><span class="toc-number">1.0.3.</span> <span class="toc-text">HTML</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Request库的使用"><span class="toc-number">2.</span> <span class="toc-text">Request库的使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BeautifulSoup-4的使用"><span class="toc-number">3.</span> <span class="toc-text">BeautifulSoup 4的使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sqlite"><span class="toc-number">4.</span> <span class="toc-text">sqlite</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#土家购模拟登录"><span class="toc-number">5.</span> <span class="toc-text">土家购模拟登录</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bilibili排行榜和动态通知"><span class="toc-number">6.</span> <span class="toc-text">Bilibili排行榜和动态通知</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Python爬虫框架-—-scrapy"><span class="toc-number">7.</span> <span class="toc-text">Python爬虫框架 — scrapy</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#并发编程-多并发爬虫"><span class="toc-number">8.</span> <span class="toc-text">并发编程, 多并发爬虫</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Python爬虫学习笔记
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">看多了会困</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2017-11-04T15:28:58.000Z" itemprop="datePublished">2017-11-04</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Python/">Python</a>, <a class="tag-link" href="/tags/爬虫/">爬虫</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>今天开始学习写爬虫吧~</p>
<a id="more"></a>

<h1 id="Python对XML-JSON-HTML的解析"><a href="#Python对XML-JSON-HTML的解析" class="headerlink" title="Python对XML,JSON,HTML的解析"></a>Python对XML,JSON,HTML的解析</h1><h3 id="xml"><a href="#xml" class="headerlink" title="xml"></a>xml</h3><p>我们知道一般的无结构文本的组织格式常见的有CSV, XML, JSON, HTML这些. 其中比较相近的是XML和HTML这两个. 他们之间最大的不同点应该就是XML中没有自闭和标签这一点了, 例如:在HTML中经常会看到<code>&lt;br/&gt;</code>这样的自己就把自己闭合的标签, 而在XML中, 这个就是非法的字符.</p>
<p>废话不说了, 直接看一个XML的解析实例: </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bookstore</span> <span class="attr">name</span>=<span class="string">"The First Bookstore"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">book</span> <span class="attr">name</span>=<span class="string">"HarryPotter"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">amount</span>&gt;</span>23<span class="tag">&lt;/<span class="name">amount</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">price</span>&gt;</span>100<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">book</span> <span class="attr">name</span>=<span class="string">"Little Prince"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">amount</span>&gt;</span>10<span class="tag">&lt;/<span class="name">amount</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">price</span>&gt;</span>88<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bookstore</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这是一个典型的XML文件 现在我们去用Python搞它:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xml.dom <span class="keyword">import</span> minidom</span><br><span class="line"></span><br><span class="line">doc = minidom.parse(<span class="string">'book.xml'</span>)</span><br><span class="line">root = doc.documentElement</span><br><span class="line">print(root.nodeName)</span><br><span class="line">print(root.getAttribute(<span class="string">'name'</span>))</span><br><span class="line">books = root.getElementsByTagName(<span class="string">"book"</span>)</span><br><span class="line"><span class="keyword">for</span> book <span class="keyword">in</span> books:</span><br><span class="line">    amounts = book.getElementsByTagName(<span class="string">"amount"</span>)</span><br><span class="line">    prices = book.getElementsByTagName(<span class="string">"price"</span>)</span><br><span class="line">    print(<span class="string">"The book "</span> + book.getAttribute(<span class="string">"name"</span>) + <span class="string">" have "</span> + amounts[<span class="number">0</span>].childNodes[<span class="number">0</span>].nodeValue + <span class="string">" left."</span>)</span><br><span class="line">    print(<span class="string">"And the price of it is "</span> + prices[<span class="number">0</span>].childNodes[<span class="number">0</span>].nodeValue)</span><br></pre></td></tr></table></figure>

<p> 其中, 我们使用DOM的方式进行解析, 从代码中也可以看出来, 这里是把整个XML文件解析结束之后(也就是装载进内存之后)才可以进行处理. 显然这种方式的有点在于他可以快速的定位到任意一个节点上, 而缺点自然就是速度慢和资源占用大了. </p>
<p>输出结果就像这样:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bookstore</span><br><span class="line">The First Bookstore</span><br><span class="line">The book HarryPotter have 23 left.</span><br><span class="line">And the price of it is 100</span><br><span class="line">The book Little Prince have 10 left.</span><br><span class="line">And the price of it is 88</span><br></pre></td></tr></table></figure>

<p>与DOM不一样的, 另一种解析方式, 叫做SAX. 他的特点就是通过Handler的方式对这个XML文件进行解析, 读到哪里处理哪里.</p>
<p>代码就像是这样:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xml.parsers.expat <span class="keyword">import</span> ParserCreate</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSaxHandler</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_element</span><span class="params">(self, name, attrs)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        print(<span class="string">'element %s, attrs: %s'</span> % (name, str(attrs)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">end_element</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        print(<span class="string">"element %s end."</span> % name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">char_data</span><span class="params">(self, val)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> val.strip():</span><br><span class="line">            print(<span class="string">"And %s's value is %s"</span> % (self.name, val))</span><br><span class="line"></span><br><span class="line">handler = DefaultSaxHandler()</span><br><span class="line">parser = ParserCreate()</span><br><span class="line">parser.StartElementHandler = handler.start_element</span><br><span class="line">parser.EndElementHandler = handler.end_element</span><br><span class="line">parser.CharacterDataHandler = handler.char_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"book.xml"</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    parser.Parse(f.read())</span><br></pre></td></tr></table></figure>

<p>我们说过Python是一个鸭子类型的语言, 就是说只要你实现了这些功能, 那么我就可以说你是什么类型的. 这里, 我们实现一个SAX的解析器, 通过三个方法, 来读取他们的节点名称, 属性列表, 节点的值, 以及最后的读取到结束的标签的动作.</p>
<p>这样通过对handler赋值, 我们就可以进行文档的解析了, 输出结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">element bookstore, attrs: &#123;&apos;name&apos;: &apos;The First Bookstore&apos;&#125;</span><br><span class="line">element book, attrs: &#123;&apos;name&apos;: &apos;HarryPotter&apos;&#125;</span><br><span class="line">element amount, attrs: &#123;&#125;</span><br><span class="line">And amount&apos;s value is 23</span><br><span class="line">element amount end.</span><br><span class="line">element price, attrs: &#123;&#125;</span><br><span class="line">And price&apos;s value is 100</span><br><span class="line">element price end.</span><br><span class="line">element book end.</span><br><span class="line">element book, attrs: &#123;&apos;name&apos;: &apos;Little Prince&apos;&#125;</span><br><span class="line">element amount, attrs: &#123;&#125;</span><br><span class="line">And amount&apos;s value is 10</span><br><span class="line">element amount end.</span><br><span class="line">element price, attrs: &#123;&#125;</span><br><span class="line">And price&apos;s value is 88</span><br><span class="line">element price end.</span><br><span class="line">element book end.</span><br><span class="line">element bookstore end.</span><br></pre></td></tr></table></figure>

<p>十分简单吧.</p>
<h3 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h3><p>我们之前其实说过了Python对JSON的支持的, 虽然篇幅不多, 但是这个模块真的不是很复杂,(起码使用起来不难). </p>
<p>比如这样的JSON文件:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"value"</span>: [&#123;</span><br><span class="line">            <span class="attr">"book"</span>: <span class="string">"HarryPort"</span>,</span><br><span class="line">            <span class="attr">"price"</span>: <span class="string">"101"</span>,</span><br><span class="line">            <span class="attr">"amount"</span>: <span class="string">"199"</span></span><br><span class="line">        &#125;,</span><br><span class="line"></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"book"</span>: <span class="string">"Little Prince"</span>,</span><br><span class="line">            <span class="attr">"price"</span>: <span class="string">"18"</span>,</span><br><span class="line">            <span class="attr">"amount"</span>: <span class="string">"123"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>python对JSON的api设计是, 如果文件操作, 那么就是<code>load</code>和<code>dump</code>. 而如果是对字符串对象的操作的话, 就是<code>loads</code>和<code>dumps</code>了. 所以针对上面的那个文件, 我们这样来解析:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">f = open(<span class="string">"book.json"</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">output = json.load(f)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>

<p>就完了. 很简单. 这里输出的output变量其实是个字典. 当然了, 我也可以从字典得到JSON字符串, 综合起来就是这样的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">f = open(<span class="string">"book.json"</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">output = json.load(f)</span><br><span class="line">print(type(output))</span><br><span class="line">print(output[<span class="string">'value'</span>])</span><br><span class="line">json_str = json.dumps(output)</span><br><span class="line">print(type(json_str))</span><br><span class="line">print(json_str)</span><br></pre></td></tr></table></figure>

<p>上面代码的输出是:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;dict&apos;&gt;</span><br><span class="line">[&#123;&apos;book&apos;: &apos;HarryPort&apos;, &apos;price&apos;: &apos;101&apos;, &apos;amount&apos;: &apos;199&apos;&#125;, &#123;&apos;book&apos;: &apos;Little Prince&apos;, &apos;price&apos;: &apos;18&apos;, &apos;amount&apos;: &apos;123&apos;&#125;]</span><br><span class="line">&lt;class &apos;str&apos;&gt;</span><br><span class="line">&#123;&quot;value&quot;: [&#123;&quot;book&quot;: &quot;HarryPort&quot;, &quot;price&quot;: &quot;101&quot;, &quot;amount&quot;: &quot;199&quot;&#125;, &#123;&quot;book&quot;: &quot;Little Prince&quot;, &quot;price&quot;: &quot;18&quot;, &quot;amount&quot;: &quot;123&quot;&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>最后我们在来说一下HTML的解析, 关于这个, 我打算直接用个例子就带过去, 原因有两个:</p>
<ul>
<li>原生的HTML Parser在实际情况下不会得到使用,  第三方库的使用概率要大的多</li>
<li>HTML Parser模块很简单, 他和前面的SAX解析XML几乎差不多.</li>
</ul>
<h3 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h3><p>首先一个Web页面:</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;<span class="name">title</span>&gt;</span>403 Forbidden<span class="tag">&lt;/<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span> <span class="attr">bgcolor</span>=<span class="string">"white"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span><span class="tag">&lt;<span class="name">h1</span>&gt;</span>403 Forbidden<span class="tag">&lt;/<span class="name">h1</span>&gt;</span><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">hr</span>/&gt;</span><span class="tag">&lt;<span class="name">center</span>&gt;</span>nginx/1.4.6 (Ubuntu)<span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>(  嘿嘿, 特意挑了一个403页面 )</p>
<p>这里我们能看到除了存在和XML一样的的前后闭合标签, 还有一个烦人的<code>&lt;hr/&gt;</code>. 现在我们来实现一下我们的Parser:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="keyword">from</span> html.parser <span class="keyword">import</span> HTMLParser</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultParser</span><span class="params">(HTMLParser)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_starttag</span><span class="params">(self, tag, attrs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tag == <span class="string">'html'</span>:</span><br><span class="line">            print(<span class="string">'Start parsing.'</span>)</span><br><span class="line">        <span class="keyword">if</span> tag == <span class="string">'title'</span>:</span><br><span class="line">            print(<span class="string">"Output title:"</span>)</span><br><span class="line">        <span class="keyword">if</span> tag == <span class="string">'h1'</span>:</span><br><span class="line">            print(<span class="string">"Output h1:"</span>)</span><br><span class="line">        <span class="keyword">if</span> attrs:</span><br><span class="line">            print(<span class="string">"%s has attr: %s"</span> % (tag, attrs))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_endtag</span><span class="params">(self, tag)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tag == <span class="string">'html'</span>:</span><br><span class="line">            print(<span class="string">"End parsing"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_startendtag</span><span class="params">(self, tag, attrs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tag == <span class="string">'hr'</span>:</span><br><span class="line">            print(<span class="string">'Found hr'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_data</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> data:</span><br><span class="line">            print(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">parser = DefaultParser()</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"index.html"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    parser.feed(f.read())</span><br><span class="line">parser.close()</span><br></pre></td></tr></table></figure>

<p>这样执行之后获得的结果是这样的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Start parsing.</span><br><span class="line">Output title:</span><br><span class="line">403 Forbidden</span><br><span class="line">body has attr: [(&apos;bgcolor&apos;, &apos;white&apos;)]</span><br><span class="line">Output h1:</span><br><span class="line">403 Forbidden</span><br><span class="line">Found hr</span><br><span class="line">nginx/1.4.6 (Ubuntu)</span><br><span class="line">End parsing</span><br></pre></td></tr></table></figure>

<p>其实, Python除了这个HTMLParser, 还有其他的用来解析HTML文档的模块, 而且事实上, 他们要更加好用. 不过也就到这里了. 现在我们来看看这一次爬虫使用到的第一个库! — <strong>request</strong>.</p>
<h1 id="Request库的使用"><a href="#Request库的使用" class="headerlink" title="Request库的使用"></a>Request库的使用</h1><p>Request是一个超级好用的Python网络库, 封装大量的urllib3中的API, 并提供一个更好用的接口给开发者. 最简单的一个请求操作就是这样的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://59.68.29.126"</span></span><br><span class="line"></span><br><span class="line">res = requests.get(url)</span><br><span class="line">print(res)</span><br><span class="line">print(res.status_code)</span><br><span class="line">print(res.headers)</span><br><span class="line">print(res.text)</span><br></pre></td></tr></table></figure>

<p>你会发现, get不就是我们的HTTP方法中的一个嘛, 事实也是如此, request的HTTP方法就是直接用的其方法名来命名的.</p>
<p>这样的请求输出的结果是这样的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;Response [403]&gt;</span><br><span class="line">403</span><br><span class="line">&#123;&apos;Server&apos;: &apos;nginx/1.4.6 (Ubuntu)&apos;, &apos;Date&apos;: &apos;Sun, 05 Nov 2017 12:07:26 GMT&apos;, &apos;Content-Type&apos;: &apos;text/html; charset=utf-8&apos;, &apos;Transfer-Encoding&apos;: &apos;chunked&apos;, &apos;Connection&apos;: &apos;keep-alive&apos;, &apos;Content-Encoding&apos;: &apos;gzip&apos;&#125;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body bgcolor=&quot;white&quot;&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx/1.4.6 (Ubuntu)&lt;/center&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>

<p>怎么样, <code>get</code>输出的response对象中携带了大量你所需要的值和结果. </p>
<p>GET请求就是这样的, 那么接下来如果我想要携带参数去请求呢:</p>
<p>请看下面的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/get'</span></span><br><span class="line">params = &#123;<span class="string">'k1'</span>: <span class="string">'v1'</span>, <span class="string">'k2'</span>: <span class="literal">None</span>, <span class="string">'k3'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;</span><br><span class="line">res = requests.get(url, params=params)</span><br><span class="line">print(res.url)</span><br></pre></td></tr></table></figure>

<p>输出的结果是:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://httpbin.org/get?k1=v1&amp;k3=1&amp;k3=2&amp;k3=3</span><br></pre></td></tr></table></figure>

<p>由于k2没有参数值, 所以直接就略去了. 而k3是个数组,  所以出现了多次.</p>
<blockquote>
<p>关于 httpbin.org这个网站, 它提供了很多很方便的功能, 例如:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt; ➜  ~ curl http://httpbin.org/user-agent</span><br><span class="line">&gt; &#123;</span><br><span class="line">&gt;   <span class="string">"user-agent"</span>: <span class="string">"curl/7.54.0"</span></span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; ➜  ~ curl http://httpbin.org/headers</span><br><span class="line">&gt; &#123;</span><br><span class="line">&gt;   <span class="string">"headers"</span>: &#123;</span><br><span class="line">&gt;     <span class="string">"Accept"</span>: <span class="string">"*/*"</span>,</span><br><span class="line">&gt;     <span class="string">"Connection"</span>: <span class="string">"close"</span>,</span><br><span class="line">&gt;     <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>,</span><br><span class="line">&gt;     <span class="string">"User-Agent"</span>: <span class="string">"curl/7.54.0"</span></span><br><span class="line">&gt;   &#125;</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>他的主页上有更多说明.</p>
</blockquote>
<p>这样就可以携带参数的请求了.</p>
<p>除了文本请求, 有时候我们可能需要去爬一些图片什么的二进制数据, 这怎么办呢? 其实这个时候和我们的Requests的滚阿西就不是很大了, 只不过是需要response对象中的一个content属性罢了, 以图片为例, 来看下面的一个实例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://www.baidu.com/img/bd_logo1.png"</span></span><br><span class="line">res = requests.get(url)</span><br><span class="line">image = Image.open(BytesIO(res.content))</span><br><span class="line">image.save(<span class="string">"baidu_logo.png"</span>)</span><br></pre></td></tr></table></figure>

<p>这样, 我们就可以进行图片的抓取了. 有的时候, 一张图有可能会很大, 这个时候, 我们可以把它当做原始数据 一个chunk一个chunk的读取, 也就是<strong>流式读取</strong>. 来看改进之后的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://www.baidu.com/img/bd_logo1.png"</span></span><br><span class="line">res = requests.get(url, stream=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"baidu_logo_stream.png"</span>, <span class="string">'wb+'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> trunk <span class="keyword">in</span> res.iter_content(<span class="number">512</span>):</span><br><span class="line">        f.write(trunk)</span><br></pre></td></tr></table></figure>

<p>GET的用法其实就是这些了, 接着我们说一下POST的用法, 最常用的场景就是提交表单了. 这个时候其实和GET携带参数差不多的用法:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://httpbin.org/post"</span></span><br><span class="line">form = &#123;<span class="string">"username"</span>: <span class="string">'Justin'</span>, <span class="string">"password"</span>: <span class="string">"ielts666"</span>&#125;</span><br><span class="line">res = requests.post(url, data=form)</span><br><span class="line">print(json.loads(res.text)[<span class="string">'form'</span>])</span><br><span class="line">res = requests.post(url, data=json.dumps(form))</span><br><span class="line">print(json.loads(res.text)[<span class="string">'form'</span>])</span><br><span class="line">print(json.loads(res.text)[<span class="string">'data'</span>])</span><br></pre></td></tr></table></figure>

<p>得到的结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&apos;password&apos;: &apos;ielts666&apos;, &apos;username&apos;: &apos;Justin&apos;&#125;</span><br><span class="line">&#123;&#125;</span><br><span class="line">&#123;&quot;username&quot;: &quot;Justin&quot;, &quot;password&quot;: &quot;ielts666&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>也就是说, 当我们post传入的data是个字段对象的时候, 数据就会填充在表单中,而如果是纯文本, 就会丢在data项目中.</p>
<p>说道了提交表单, 我们还要说一下有关Cookie的一些, 首先先用必应做个实验: (因为它的Cookie比较多 - -)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://bing.com"</span></span><br><span class="line">res = requests.get(url)</span><br><span class="line">print(res.cookies.items())</span><br></pre></td></tr></table></figure>

<p>结果是这样的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;MUID&apos;, &apos;270542D09F6B6A1B05D349E29EB76BFB&apos;), (&apos;SRCHD&apos;, &apos;AF=NOFORM&apos;), (&apos;SRCHUID&apos;, &apos;V=2&amp;GUID=0FC639A2A7F7477EAD9CC949860A8CBD&amp;dmnchg=1&apos;), (&apos;SRCHUSR&apos;, &apos;DOB=20171106&apos;), (&apos;_EDGE_S&apos;, &apos;F=1&amp;SID=0E4DEFBCD35565913449E48ED28964EC&apos;), (&apos;_EDGE_V&apos;, &apos;1&apos;), (&apos;_SS&apos;, &apos;SID=0E4DEFBCD35565913449E48ED28964EC&apos;), (&apos;MUIDB&apos;, &apos;270542D09F6B6A1B05D349E29EB76BFB&apos;)]</span><br></pre></td></tr></table></figure>

<p>当然我们也可以进行遍历, 进行进一步的操作.</p>
<p>同样, 我们也可以携带Cookie进行请求, 就像这样:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">"http://httpbin.org/cookies"</span></span><br><span class="line">cookie = &#123;<span class="string">"name"</span>: <span class="string">'Justin'</span>&#125;</span><br><span class="line">res = requests.get(url, cookies=cookie)</span><br><span class="line">print(res.text)</span><br></pre></td></tr></table></figure>

<p>我们的Cookie确实送过去了:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;cookies&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;Justin&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Requests同样可以做到重定向以及重定向的历史:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">"http://justin13wyx.me"</span></span><br><span class="line">res = requests.get(url, allow_redirects=<span class="literal">True</span>)</span><br><span class="line">print(res.url)</span><br><span class="line">print(res.status_code)</span><br><span class="line">print(res.history[<span class="number">0</span>].status_code)</span><br></pre></td></tr></table></figure>

<p>输出结果是:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://justin13wyx.me//</span><br><span class="line">200</span><br><span class="line">302</span><br></pre></td></tr></table></figure>

<p>最后关于Requests的基本功能, 我们再说一个使用代理:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">"http://httpbin.org"</span></span><br><span class="line">proxies = &#123;<span class="string">"http"</span>: <span class="string">"假装有个代理服务器"</span>, <span class="string">"https"</span>: <span class="string">"假装这里有个代理服务器"</span>&#125;</span><br><span class="line">res = requests.get(url, proxies=proxies)</span><br></pre></td></tr></table></figure>

<p>这样就差不多了.</p>
<h1 id="BeautifulSoup-4的使用"><a href="#BeautifulSoup-4的使用" class="headerlink" title="BeautifulSoup 4的使用"></a>BeautifulSoup 4的使用</h1><p>我们提供一个示例文档:</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Sample File<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--body_start--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"para"</span>&gt;</span>This is a sample file for beautiful soup4<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://www.baidu.com"</span>&gt;</span>Click Here<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;<span class="name">p</span>&gt;</span>And you will see the homepage of baidu</span><br><span class="line">    <span class="tag">&lt;<span class="name">em</span> <span class="attr">id</span>=<span class="string">"em"</span>&gt;</span>BAIDU<span class="tag">&lt;/<span class="name">em</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>last para<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>可以很清楚的看出来, 它的排版并不是很好看(甚至还有错误[body没有闭合]). 接下来我们就可以使用BS来做一下处理:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(open(<span class="string">"bs_sample.html"</span>), <span class="string">"html.parser"</span>)</span><br><span class="line">print(soup.prettify())</span><br></pre></td></tr></table></figure>

<p>输出的文本就是按照层级结构排过版的了 同时, body标签也被自动的补全了.</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"utf-8"</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span></span><br><span class="line">   Sample File</span><br><span class="line">  <span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--body_start--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">"para"</span>&gt;</span></span><br><span class="line">   This is a sample file for beautiful soup4</span><br><span class="line">  <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://www.baidu.com"</span>&gt;</span></span><br><span class="line">   Click Here</span><br><span class="line">  <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">   And you will see the homepage of baidu</span><br><span class="line">   <span class="tag">&lt;<span class="name">em</span> <span class="attr">id</span>=<span class="string">"em"</span>&gt;</span></span><br><span class="line">    BAIDU</span><br><span class="line">   <span class="tag">&lt;/<span class="name">em</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">   last para</span><br><span class="line">  <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>接下来就是BS的强大的地方了, 我们可以非常方便的读取文档, 就像下面这样:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(open(<span class="string">"bs_sample.html"</span>), <span class="string">"html.parser"</span>)</span><br><span class="line"><span class="comment"># print(soup.prettify())</span></span><br><span class="line"></span><br><span class="line">print(type(soup.title))</span><br><span class="line">print(soup.title.text)</span><br><span class="line">print(soup.p)</span><br></pre></td></tr></table></figure>

<p>输出的结果是这个样子的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;bs4.element.Tag&apos;&gt;</span><br><span class="line">Sample File</span><br><span class="line">&lt;p class=&quot;para&quot;&gt;This is a sample file for beautiful soup4&lt;/p&gt;</span><br></pre></td></tr></table></figure>

<p>但是 问题出来了, 这里抓取到的元素都是第一个, 后面的元素好像没法直接获得. 所以这个时候, 我们就需要进行遍历了:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(open(<span class="string">"bs_sample.html"</span>), <span class="string">"html.parser"</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> soup.body.contents:</span><br><span class="line">    print(item.name)</span><br></pre></td></tr></table></figure>

<p>输出的结果有点奇怪:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">None</span><br><span class="line">None</span><br><span class="line">None</span><br><span class="line">p</span><br><span class="line">None</span><br><span class="line">a</span><br><span class="line">p</span><br><span class="line">None</span><br><span class="line">p</span><br><span class="line">None</span><br></pre></td></tr></table></figure>

<p> 这是因为papp这四个元素是body下的直接子元素.</p>
<p>接下来我们看一下BS的CSS查询:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(soup.select(<span class="string">".para"</span>))</span><br></pre></td></tr></table></figure>

<p>这样我就得到了:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&lt;p class=&quot;para&quot;&gt;This is a sample file for beautiful soup4&lt;/p&gt;]</span><br></pre></td></tr></table></figure>

<p>这样一个元素列表.</p>
<p>当然id查询也是可以的, 就不演示了. 当然也是支持这样的层级查找:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(soup.select(<span class="string">"body &gt; p.para"</span>))</span><br></pre></td></tr></table></figure>

<p>很简单, 更多使用到时候在实战中使用再说. 现在我们来看一下Python &gt; sqlite的使用</p>
<h1 id="sqlite"><a href="#sqlite" class="headerlink" title="sqlite"></a>sqlite</h1><p> Python方便的地方就是他自带了sqlite, 这很方便, 而且SQL语句的使用也是很标准的, 所以如果你使用过MySQL, 那么使用sqlite模块也是手到拈来的事情了:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line">conn = sqlite3.connect(<span class="string">"test.db"</span>)</span><br><span class="line">create_sql = <span class="string">"create table if not exists test(id int primary key not null, name varchar(20) not null)"</span></span><br><span class="line">conn.execute(create_sql)</span><br><span class="line">insert_sql = <span class="string">"insert into test values(?, ?)"</span></span><br><span class="line">conn.execute(insert_sql, (<span class="number">1</span>, <span class="string">"test"</span>))</span><br><span class="line">conn.execute(insert_sql, (<span class="number">2</span>, <span class="string">"test"</span>))</span><br><span class="line">get_sql = <span class="string">"select * from test where name = ?"</span></span><br><span class="line">res = conn.execute(get_sql, (<span class="string">"test"</span>,))</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> res.fetchall():</span><br><span class="line">    print(item[<span class="number">1</span>])</span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure>

<p>这样输出的结果就是:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test</span><br><span class="line">test</span><br></pre></td></tr></table></figure>

<p>好了, 准备的额差不多了, 接下来我就来试着模拟登陆一下好了.</p>
<h1 id="土家购模拟登录"><a href="#土家购模拟登录" class="headerlink" title="土家购模拟登录"></a>土家购模拟登录</h1><p>不管写什么爬虫, 我们都需要先观察对方网站是怎么处理的请求, 怎么发送的表单, 土家购的登录十分简单, 直接使用浏览器的开发者工具就可以了: ( 别问我土家购是什么 - - )</p>
<p><img src="http://hexopic.s3-website-ap-northeast-1.amazonaws.com/tujiagou_login.png" alt="tujiagou_login.png"></p>
<p>十分好看.</p>
<p>接下来我就要来模拟一下登陆过程了, 直接上个代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">s = requests.Session()</span><br><span class="line"></span><br><span class="line">login_url = <span class="string">"http://tjg.hangowa.com/member/?act=login&amp;op=login&amp;inajax=1"</span></span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>:</span><br><span class="line">               <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 \</span></span><br><span class="line"><span class="string">               (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span>,</span><br><span class="line">           <span class="string">"Referer"</span>:</span><br><span class="line">               <span class="string">"http://tjg.hangowa.com/member/?act=login&amp;op=index&amp;ref_url=%2Fmember \</span></span><br><span class="line"><span class="string">               %2F%3Fact%3Dmember_information%26op%3Dindex"</span>&#125;</span><br><span class="line"></span><br><span class="line">formdata = &#123;</span><br><span class="line">    <span class="string">"formhash"</span>: <span class="string">"gjn1eYx_4djUfHKPpQAhjX0eYT0fEDP"</span>,</span><br><span class="line">    <span class="string">"form_submit"</span>: <span class="string">"ok"</span>,</span><br><span class="line">    <span class="string">"nchash"</span>: <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"user_name"</span>: <span class="string">"user"</span>,</span><br><span class="line">    <span class="string">"password"</span>: <span class="string">"不告诉你"</span>,</span><br><span class="line">    <span class="string">"ref_url"</span>: <span class="string">"/member/?act=member_information&amp;op=index"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">res = s.post(login_url, data=formdata, headers=headers)</span><br><span class="line">home = s.get(<span class="string">"http://tjg.hangowa.com/shop/index.php?act=cart"</span>)</span><br><span class="line">soup = BeautifulSoup(home.content, <span class="string">"html5lib"</span>)</span><br><span class="line">goods = soup.select(<span class="string">"td &gt; a &gt; img"</span>)</span><br><span class="line"><span class="keyword">for</span> good <span class="keyword">in</span> goods:</span><br><span class="line">    print(good.get(<span class="string">"alt"</span>))</span><br><span class="line"></span><br><span class="line">s.close()</span><br></pre></td></tr></table></figure>

<p>这样就可以获得我的购物车的商品了, 结果如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">骑龙烘青9号50g绿茶</span><br><span class="line">恩施鹤峰骑龙绿茶250g袋装</span><br><span class="line">恩施鹤峰青翠源黑茶400g</span><br><span class="line">斑鹤黄金白茶50g*2一盒二袋</span><br><span class="line">恩施鹤峰斑鹤250g绿茶</span><br><span class="line">%name%</span><br></pre></td></tr></table></figure>

<p>是不是很简单呢, 当然. 这也是因为土家购对登录没有做什么验证啥的, 所以很轻松的就登陆进去了.</p>
<p>接着我们可以使用开发者工具把自己登陆之后的Cookie复制下来, 只要对方没有设置其他的干扰项, 我们就可以是哟ing你Cookie进行免密码的登陆进去. </p>
<p>接着来看这两个小栗子:</p>
<h1 id="Bilibili排行榜和动态通知"><a href="#Bilibili排行榜和动态通知" class="headerlink" title="Bilibili排行榜和动态通知"></a>Bilibili排行榜和动态通知</h1><p>直接上代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span>,</span><br><span class="line">    <span class="string">"Referer"</span>: <span class="string">"https://www.bilibili.com/ranking"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://www.bilibili.com/index/rank/all-3-0.json"</span></span><br><span class="line"></span><br><span class="line">rank_res = requests.get(url, headers=headers)</span><br><span class="line">rank_json = json.loads(rank_res.text)</span><br><span class="line">location = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> rank_json[<span class="string">'rank'</span>][<span class="string">'list'</span>]:</span><br><span class="line">    print(<span class="string">"第%s名: %s Up:%s\n播放数:%s 硬币:%s, 弹幕:%s"</span> % (str(location), item[<span class="string">'title'</span>], item[<span class="string">'author'</span>], item[<span class="string">'play'</span>], item[<span class="string">'coins'</span>], item[<span class="string">'video_review'</span>]))</span><br><span class="line">    location += <span class="number">1</span>;</span><br><span class="line">    print(<span class="string">"-"</span> * <span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<p>运行结果就像这样:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">第1名: 【凉风】不正经解说柯南剧场版M21-唐红的恋歌 Up:凉风有性胖次君</span><br><span class="line">播放数:484836 硬币:30721, 弹幕:5670</span><br><span class="line">--------------------</span><br><span class="line">第2名: 诸葛亮的红色蜀国 Up:画画峰</span><br><span class="line">播放数:822549 硬币:60046, 弹幕:7214</span><br><span class="line">--------------------</span><br><span class="line">第3名: 【马云】王妃：我的baby，我要霸占你的钱！ Up:深海色带鱼</span><br><span class="line">播放数:521766 硬币:41325, 弹幕:3235</span><br><span class="line">--------------------</span><br><span class="line">第4名: 【C菌】首款赛博朋克风恐怖神作!【&gt;Observer_ (观察者)】实况连载, 更新第五集 Up:渗透之C君</span><br><span class="line">播放数:398717 硬币:21198, 弹幕:11739</span><br><span class="line">--------------------</span><br><span class="line">第5名: 【兄贵】病名为爱♂ Up:小可儿</span><br><span class="line">播放数:465863 硬币:55400, 弹幕:7637</span><br><span class="line">--------------------</span><br><span class="line">第6名: 死亡？蜘蛛侠：仅剩之日 Up:努力的Lorre</span><br><span class="line">播放数:557282 硬币:60271, 弹幕:8679</span><br><span class="line">--------------------</span><br><span class="line">第7名: 正气凛然！对美少女的丝袜诱惑说不！【二次元观察室04】 Up:吃素的狮子</span><br><span class="line">播放数:220998 硬币:31966, 弹幕:10283</span><br><span class="line">--------------------</span><br><span class="line">第8名: 《守望先锋》动画短片：《荣耀》 Up:网易暴雪游戏视频</span><br><span class="line">播放数:536341 硬币:22895, 弹幕:5294</span><br><span class="line">--------------------</span><br><span class="line">第9名: 【原作党很严格】为什么动画都喜欢魔改剧情？泛式带你走进魔改党背后的真相 Up:泛式</span><br><span class="line">播放数:237893 硬币:23400, 弹幕:3576</span><br><span class="line">...(omitted)</span><br></pre></td></tr></table></figure>

<p>很有意思吧, 另外, 使用你登录之后的Cookie就可以直接进行动态的抓取了, 代码如下(Cookie需要替换):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://api.bilibili.com/x/web-feed/feed?callback=jQuery172020613184013553032_1510060069102&amp;jsonp=jsonp&amp;ps=10&amp;type=0&amp;_=1510060069292"</span></span><br><span class="line"></span><br><span class="line">cookie = &#123;</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">"你的Cookie"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line">    <span class="string">'Referer'</span>: <span class="string">"https://www.bilibili.com/account/dynamic"</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.75 Safari/537.36"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">res = requests.get(url, cookies=cookie, headers=header)</span><br><span class="line">result = res.text</span><br><span class="line"></span><br><span class="line">pattern = <span class="string">"jQuery[0-9_]*\((.*)\)"</span></span><br><span class="line"></span><br><span class="line">dynamic = json.loads(re.sub(pattern, <span class="string">r'\1'</span>, result))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dynamic[<span class="string">'data'</span>]:</span><br><span class="line">    archive = data[<span class="string">'archive'</span>]</span><br><span class="line">    bangumi = data[<span class="string">'bangumi'</span>]</span><br><span class="line">    print(<span class="string">"--"</span> * <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">if</span> archive:</span><br><span class="line">        print(<span class="string">"发布时间: %s Up: %s 标题: %s\n%s"</span> % (datetime.fromtimestamp(float(data[<span class="string">'pubdate'</span>])), archive[<span class="string">'owner'</span>][<span class="string">'name'</span>], archive[<span class="string">'title'</span>], archive[<span class="string">'desc'</span>]))</span><br><span class="line">    <span class="keyword">if</span> bangumi:</span><br><span class="line">        print(<span class="string">"**我关注的番剧更新了!**\n**%s**\n**第%s话: %s**"</span> % (bangumi[<span class="string">'title'</span>], bangumi[<span class="string">'new_ep'</span>][<span class="string">'index'</span>], bangumi[<span class="string">'new_ep'</span>][<span class="string">'index_title'</span>]))</span><br></pre></td></tr></table></figure>

<p>运行的结果就像这样:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">--------------------</span><br><span class="line">发布时间: 2017-11-07 20:40:25 Up: 刘哔电影 标题: 【刘哔】我不扶墙就服这支广告的设计</span><br><span class="line">京东今年大牌云集，还让外卖小哥抢了戏？我不扶墙就服这支广告的设计，会引起的某些欲望，观看要谨慎，京东11.11 全球好物节，千万别忘了我们的约定！</span><br><span class="line">--------------------</span><br><span class="line">发布时间: 2017-11-06 18:20:23 Up: 刘哔电影 标题: 【刘哔】烂片吐槽之《青春期3》：你确定你过的是青年期 不是更年期吗？</span><br><span class="line">明明是更年期！</span><br><span class="line">--------------------</span><br><span class="line">**我关注的番剧更新了!**</span><br><span class="line">**宝石之国**</span><br><span class="line">**第5话: 归来**</span><br><span class="line">--------------------</span><br><span class="line">发布时间: 2017-11-04 15:53:19 Up: 泛式 标题: 【原作党很严格】为什么动画都喜欢魔改剧情？泛式带你走进魔改党背后的真相</span><br><span class="line">视频类型: 动漫杂谈</span><br><span class="line">相关题材: 欢迎来到实力至上主义的教室，血界战线，电玩咖，中二病也要谈恋爱，噬魂师，火影忍者，甘城光辉游乐园，银魂，食灵，钢之炼金术师，Clannad，旋风管家</span><br><span class="line">简介: 近来魔改一词出现得越来越频繁，为什么动画组总是喜欢教原作党做人？今天就来和大家聊一聊动画化时的剧情魔改现象吧。 （所有论点系业余言论，仅供娱乐）</span><br><span class="line"></span><br><span class="line">微博：https://weibo.com/FunShiki</span><br><span class="line">--------------------</span><br><span class="line">发布时间: 2017-11-04 14:12:37 Up: 凉风有性胖次君 标题: 【凉风】不正经解说柯南剧场版M21-唐红的恋歌</span><br><span class="line">视频类型: 动漫杂谈</span><br><span class="line">相关题材: 名侦探柯南</span><br><span class="line">简介: 这里什么都有→微博@凉风有性胖次君 http://weibo.com/10112015</span><br><span class="line">--------------------</span><br><span class="line">发布时间: 2017-11-03 19:00:55 Up: 刘哔电影 标题: 【刘哔】三分钟带你看小品改编电影《兄弟，别闹！》</span><br><span class="line">11.10～带上喜欢的妹子一起去看吧！</span><br><span class="line">--------------------</span><br><span class="line">发布时间: 2017-11-03 16:46:26 Up: 逍遥散人 标题: 【散人】穿越尉迟恭 把妹不轻松 手机网聊版</span><br><span class="line">相关游戏: 尉迟恭是什么鬼</span><br><span class="line">简介补充: 游戏名尉迟恭是什么鬼网聊版</span><br><span class="line">录制出现点问题，所以最开始介绍游戏的话语没录上，直接开场。</span><br><span class="line">玩了玩超级有趣，继续做上来大家看个乐。</span><br><span class="line">--------------------</span><br><span class="line">发布时间: 2017-11-03 03:47:12 Up: 刘哔电影 标题: 温情解说之《追缉炸弹客》：最了解你的不是枕边人，而是你的敌人</span><br><span class="line">最了解你的不是枕边人，而是你的敌人</span><br><span class="line">--------------------</span><br><span class="line">发布时间: 2017-11-02 18:30:57 Up: 还有一天就放假了 标题: 【波澜哥】爱澜澜</span><br><span class="line">小红红：五句啊，五句要是再唱不准你就娶我，口亨！</span><br><span class="line">--------------------</span><br><span class="line">发布时间: 2017-11-02 16:31:24 Up: LexBurner 标题: 【Lex吐槽】刀剑神域序列之争，我永远喜欢亚丝娜！</span><br><span class="line">视频类型: 动漫杂谈</span><br><span class="line">相关题材: 刀剑神域序列之争</span><br><span class="line">简介: 新浪微博@LexBurner</span><br></pre></td></tr></table></figure>

<p>(哎呀, 你们看到我关注的UP了..嘿嘿)</p>
<h1 id="Python爬虫框架-—-scrapy"><a href="#Python爬虫框架-—-scrapy" class="headerlink" title="Python爬虫框架 — scrapy"></a>Python爬虫框架 — scrapy</h1><p>但是, 如果说在生产环境中使用脚本一个页面一个页面的下载, 解析实在是太没有逻辑了, 而且没有效率. 而且实现起来的难度较大, 于是, 框架就这样诞生了. 其中<strong><code>scrapy</code></strong>就是这些框架中一个十分有名的一个.</p>
<p>来看一下他的架构:<br><img src="http://images2015.cnblogs.com/blog/953786/201606/953786-20160612162715136-1486217043.jpg" alt="Scrapy"></p>
<p>大体上也能搞清楚个大概, Scrapy的工作模式, 接下来我们就来说说这个架构, 最显眼的就是右边的那个下载器了, 他做的事情很单纯, 就是把内容下载过来交给Spider组件, 只要有请求过来, 下载器就会自动的去下载. 而下载的东西去哪里了呢? Spider组件将下载器给他的资源统一输出到输出管道上, 这个管道的另一端是谁呢? 不一定了, 有可能是文件, 有可能是数据库等等. 总之就是你最后期望得到的输出目标. 那么下载器的请求来源是哪里? 自然就是上面的那个调度器了, 由Spiders组件将需要请求的连接地址发送给调度器, 接着根据当前爬虫的状态动态的调度请求, 这就是Scheduler的工作.</p>
<p> 关于scrapy本身, 我们就先说道这里, 接着我们来看一个小例子: 抓取七月在线的课程:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /usr/bin/env python3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"Test"</span></span><br><span class="line">    start_urls = [<span class="string">"https://www.julyedu.com/category/index"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="course_info_box"]'</span>):</span><br><span class="line">            title = item.xpath(<span class="string">'a/h4/text()'</span>).extract_first()</span><br><span class="line">            desc = item.xpath(<span class="string">'a/p[@class="course-info-tip"][1]/text()'</span>).extract_first()</span><br><span class="line">            time = item.xpath(<span class="string">'a/p[@class="course-info-tip info-time"]/text()'</span>).extract_first()</span><br><span class="line">            res = &#123;</span><br><span class="line">                <span class="string">"title"</span>: title,</span><br><span class="line">                <span class="string">"desc"</span>: desc,</span><br><span class="line">                <span class="string">"time"</span>: time</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">yield</span> res</span><br></pre></td></tr></table></figure>

<p>怎么执行呢, 我们可以使用scrapy提供的命令行工具来执行它, 就像是这样</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crapy runspider scrapy_test.py -o scrapy_test.json</span><br></pre></td></tr></table></figure>

<p>其中<code>-o</code>参数可以将<code>yield</code>出去的结果输出到文件中 要求这个结果必须是基本类型的字典或者None, 以及Scrapy的BaseItem和Request类.</p>
<p>我们现在来看一下上面的那个小例子, 首先我们继承一个爬虫类, 这个类中我们必须要提供的是这个爬虫的名字也就是name类变量和需要爬的urls, 这个是一个列表, 由于上面的例子是个单页测试, 所以就是一个单独的页面. 接着是我们需要我们爬虫抓取的内容如何获取, 如何去解析资源的方法.</p>
<p>现在我们稍微爬去多一点资源, 这一次我们主要是采取URL拼接的方式来爬的: 一个爬取博客园文章和推荐数的例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"cnblog"</span></span><br><span class="line">    start_urls = [<span class="string">"https://www.cnblogs.com/pick/#p%s"</span> % p <span class="keyword">for</span> p <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">11</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="post_item"]'</span>):</span><br><span class="line">            title = item.xpath(<span class="string">'div[@class="post_item_body"]/h3/a/text()'</span>).extract_first()</span><br><span class="line">            recommand = item.xpath(<span class="string">'div[@class="digg"]/div/span/text()'</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">"title"</span>: title,</span><br><span class="line">                <span class="string">"rec_no"</span>: recommand</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>

<p>注意这里所使用的start_urls, 由于我们可以从URL中发现规律, 这样就可以进行分页的抓取了. 另外还有一种, 就是手动的去请求下一页, 你也可以理解成是重新请求一个新的URL啦, 我们来看一个例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuoteSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quoteSpider"</span></span><br><span class="line">    start_urls = [<span class="string">"http://quotes.toscrape.com/tag/life/"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> response.xpath(<span class="string">"//div[@class='quote']"</span>):</span><br><span class="line">            content = item.xpath(<span class="string">"span[@class='text']/text()"</span>).extract_first()</span><br><span class="line">            author = item.xpath(<span class="string">"span[2]/small/text()"</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">"content"</span>: content,</span><br><span class="line">                <span class="string">"author"</span>: author</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        next_page = response.xpath(<span class="string">"//li[@class='next']/a/@href"</span>).extract_first()</span><br><span class="line">        <span class="keyword">if</span> next_page:</span><br><span class="line">            next_page = response.urljoin(next_page)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_page, callback=self.parse)</span><br></pre></td></tr></table></figure>

<p>这里我们尝试获取页面上的下一页, 如果存在就继续请求. 由于获取到的href目标是个相对地址, 所以我们还需要进行一次URL的拼接. </p>
<p>到现在为止, 我们都在使用scrapy的单文件, 但是事实上我们一个爬虫都是有一个项目来驱动的, 使用scrapy创建一个project是十分简单的:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">➜  scrapy startproject qqnews</span><br><span class="line">New Scrapy project <span class="string">'qqnews'</span>, using template directory <span class="string">'/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/templates/project'</span>, created <span class="keyword">in</span>:</span><br><span class="line">    /.../qqnews</span><br><span class="line"></span><br><span class="line">You can start your first spider with:</span><br><span class="line">    <span class="built_in">cd</span> qqnews</span><br><span class="line">    scrapy genspider example example.com</span><br><span class="line">➜  tree qqnews</span><br><span class="line">qqnews</span><br><span class="line">├── qqnews</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── middlewares.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── settings.py</span><br><span class="line">│   └── spiders</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       └── __pycache__</span><br><span class="line">└── scrapy.cfg</span><br><span class="line"></span><br><span class="line">4 directories, 7 files</span><br></pre></td></tr></table></figure>

<p>而且我们的爬虫就在这个spider目录下创建:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  vim qqnews/qqnews/spiders/qq_news_spider.py</span><br></pre></td></tr></table></figure>

<p> 接着我们就可以去写爬虫了…</p>
<blockquote>
<p><strong>脱坑指南</strong> 如果你爬取不了(比如说被拒绝, 或者超时, 或者一直没反应), 那么尝试一下将setting.py里面的<code>ROBOTSTXT_OBEY</code>改成<code>False</code>. 这样我们的scrapy就不会遵循robots规则了.</p>
<p>另外, 腾讯新闻的渲染好坑啊, 他竟然是把所有的内容都扔到了前面的一个script里面的__initData变量里面(unicode编码)</p>
<p><strong>12.08的更新: 腾讯新闻居然改版了?? 下面的代码不再适用.</strong></p>
</blockquote>
<p>在这个过程中, 我们可以使用一个scrapy提供的调试工具来帮助我们更好的查看错误出在什么地方.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; scrapy shell http://view.inews.qq.com/a/20171125A07ZUK00</span><br><span class="line">...(omitted)</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1041e7550&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http://view.inews.qq.com/a/20171125A07ZUK00&gt;</span><br><span class="line">[s]   response   &lt;200 http://view.inews.qq.com/a/20171125A07ZUK00&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x105233940&gt;</span><br><span class="line">[s]   spider     &lt;DefaultSpider <span class="string">'default'</span> at 0x10550f668&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   fetch(url[, redirect=True]) Fetch URL and update <span class="built_in">local</span> objects (by default, redirects are followed)</span><br><span class="line">[s]   fetch(req)                  Fetch a scrapy.Request and update <span class="built_in">local</span> objects</span><br><span class="line">[s]   shelp()           Shell <span class="built_in">help</span> (<span class="built_in">print</span> this <span class="built_in">help</span>)</span><br><span class="line">[s]   view(response)    View response <span class="keyword">in</span> a browser</span><br></pre></td></tr></table></figure>

<p>接着就来上代码吧:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /usr/bin/env python3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NewsSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"qqnews"</span></span><br><span class="line">    start_urls = [<span class="string">"http://news.qq.com/world_index.shtml"</span>]</span><br><span class="line">    cookie = &#123;</span><br><span class="line">        <span class="string">"pgv_pvi"</span>: <span class="string">"6074546176"</span>,</span><br><span class="line">        <span class="string">"RK"</span>: <span class="string">"Kbk2ExxeFH"</span>,</span><br><span class="line">        <span class="string">"pgv_pvid"</span>: <span class="string">"6264596920"</span>,</span><br><span class="line">        <span class="string">"ptcz"</span>: <span class="string">"252b68d3ef2d7ecb0281a23b9f3f2e7c69a5f0d7427c7245392836431c89ce58"</span>,</span><br><span class="line">        <span class="string">"pt2gguin"</span>: <span class="string">"o0438425627"</span></span><br><span class="line">    &#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6)  \</span></span><br><span class="line"><span class="string">        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36"</span>,</span><br><span class="line">        <span class="string">"Upgrade-Insecure-Requests"</span>: <span class="string">"1"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        hotspot = response.xpath(<span class="string">'//div[@id="subHot"]/a/@href'</span>).extract_first()</span><br><span class="line">        news = response.xpath(<span class="string">'//div[@class="list first"]/div/div/div/em/a/@href'</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> news:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(item, callback=self.news_parse, cookies=self.cookie, headers=self.headers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">news_parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        pattern = <span class="string">r'"cnt_html":"(.*)","content"'</span></span><br><span class="line">        raw_content = response.xpath(<span class="string">'/html/body/script[3]/text()'</span>).extract_first().strip()</span><br><span class="line">        res = re.search(pattern, raw_content).groups()[<span class="number">0</span>].encode().decode(<span class="string">'unicode_escape'</span>)</span><br><span class="line">        print(res)</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'article_html'</span>: res</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p>好了, 但是到现在 我们都只是蠢蠢的在yield和print对不, 但是对于一个项目来说 我们是需要一个Item来把他们保存下来的. 这就是scrapy的Item, 注意到子安我们的项目目录中中是存在一个叫做<code>item.py</code>的文件的, 我们在这里面写上需要保存的项目:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QqnewsItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    article = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>接着在爬虫中引入它并给他附上值就行了:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> qqnews.iterms <span class="keyword">import</span> QqnewsItem</span><br><span class="line">...(omitted)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">news_parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    	item = QqnewsItem()</span><br><span class="line">        pattern = <span class="string">r'"cnt_html":"(.*)","content"'</span></span><br><span class="line">        raw_content = response.xpath(<span class="string">'/html/body/script[3]/text()'</span>).extract_first().strip()</span><br><span class="line">        res = re.search(pattern, raw_content).groups()[<span class="number">0</span>].encode().decode(<span class="string">'unicode_escape'</span>)</span><br><span class="line">       	item[<span class="string">'article'</span>] = res</span><br></pre></td></tr></table></figure>

<p>接着我们的item还可以继续输送到pipeline中, pipeline正如其名, 将输送进来的数据进行美化加工. 如果有点乱了的话, 不妨看一下前面的架构图</p>
<p>Pipeline常见的应用场景是:</p>
<ul>
<li>清理HTML数据</li>
<li>抓取关键字</li>
<li>重复性检查</li>
<li>存储到DB中</li>
</ul>
<p>我们来看一下这个文件:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QqnewsPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span> <span class="comment"># 这里传进来的item就是刚刚说的那个啦, 我们可以在这里提取他的字段进行修改和再赋值.</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>return一个item或者字典, 还可以抛出</p>
<p>其实, 我们可以通过覆盖一下的几个方法来继续更多的操作</p>
<ul>
<li>open_crawler 比如我们打开文件, 打开数据库链接都是在这里搞</li>
<li>close_crawler close掉链接和文件描述符</li>
<li>from_crawler 在这个方法中, 我们可以绑定各种钩子以及访问核心组件 信号和配置等</li>
</ul>
<p>接下来我打算写一个豆瓣读书的Top250这样的项目, 这一次就会综合上面的各个组件了.</p>
<p>首先启动项目:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  scrapy startproject douban_book</span><br></pre></td></tr></table></figure>

<p>接着我们先来定义我们需要抓取的信息有哪些, 然后先把他们呢注册一下.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubanBookItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    edition_year = scrapy.Field()</span><br><span class="line">    publisher = scrapy.Field()</span><br><span class="line">    rating = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>接着就可以开始定义我们的爬虫了:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> douban_book.items <span class="keyword">import</span> DoubanBookItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6)  \</span></span><br><span class="line"><span class="string">    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36"</span>,</span><br><span class="line">    <span class="string">"Referer"</span>: <span class="string">"https://book.douban.com/"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubanSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"douban_book_Spider"</span></span><br><span class="line">    allowed_domains = [<span class="string">"douban.com"</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://book.douban.com/top250'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(response)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(response.url, callback=self.page_parse, headers=header)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="paginator"]/a/@href'</span>).extract():</span><br><span class="line">            print(link)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(link, callback=self.page_parse, headers=header)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">page_parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> response.xpath(<span class="string">'//tr[@class="item"]'</span>):</span><br><span class="line">            book = DoubanBookItem()</span><br><span class="line">            book[<span class="string">'name'</span>] = item.xpath(<span class="string">'td[2]/div[1]/a/@title'</span>).extract_first()</span><br><span class="line">            book[<span class="string">'rating'</span>] = item.xpath(<span class="string">'td[2]/div[2]/span[2]/text()'</span>).extract_first()</span><br><span class="line">            info = item.xpath(<span class="string">'td[2]/p[1]/text()'</span>).extract_first().split(<span class="string">" / "</span>)</span><br><span class="line">            book[<span class="string">'author'</span>] = info[<span class="number">0</span>]</span><br><span class="line">            book[<span class="string">'price'</span>] = info[<span class="number">-1</span> ]</span><br><span class="line">            book[<span class="string">'edition_year'</span>] = info[<span class="number">-2</span>]</span><br><span class="line">            book[<span class="string">'publisher'</span>] = info[<span class="number">-3</span>]</span><br><span class="line">            <span class="keyword">yield</span> book</span><br></pre></td></tr></table></figure>

<p>到此, 我们就完成了一个包含item的爬虫. 但是这个地方要说明一下, 第29-33行的工作其实严格上应该放在pipeline中来处理, 而不是在我们的爬虫中进行处理. 优化后这就是一个较为完整的爬虫项目了 虽然他很微小.</p>
<p>接下来我们来说一下如何抓取图片, 这是一个很大的需求, 所以scrapy早就为我们想好了, 它提供了一个<code>ImagesPipeline</code>类, 只要将一个item传过去, 并且携带特定的字段就可以继续操作, 包括基本的去重, 缩略图生成, 图片大小过滤. 当然这是需要PIL库的支持的.</p>
<p>大体说来, 我们的图片爬取步骤几乎是固定的, 就是这样的:</p>
<ul>
<li>爬取一个item, 并且将图片的URL加入到其<code>image_urls</code>字段中</li>
<li>从Spider中返回的item送到pipeline中, 图片在这里会被下载而item被锁定直到下载完成, 最后剩余的一些信息(路径, 校验和等)会被传递到item的相应字段中.</li>
</ul>
<p>一个图片item起码要包含:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PictureItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    images = scrapy.Field()</span><br><span class="line">    image_urls = scrapy.Field()</span><br><span class="line">    image_paths = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p><code>images</code>和<code>image_urls</code>这两个属性.</p>
<p>另外, 写好pipeline之后, 我们需要在<code>setting.py</code>文件中开启该pipeline, 并且制定一些图片下载的全局总控参数, 例如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'photo_crawler.pipelines.PhotoCrawlerPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">IMAGES_STORE = <span class="string">'/tmp/pics'</span></span><br><span class="line">IMAGES_EXPIRES = <span class="number">90</span></span><br><span class="line">IMAGES_MIN_HEIGHT = <span class="number">480</span></span><br><span class="line">IMAGES_MIN_WIDTH = <span class="number">920</span></span><br></pre></td></tr></table></figure>

<p>尺寸大小小于<code>IMAGES_MIN_HEIGHT*IMAGES_MIN_WIDTH</code>的图片会被忽略.</p>
<p>接着在spider中, 我们需要通过解析把图片的url都丢到item中, 传给pipeline.</p>
<p>最重要的部分来了, 我们需要在<code>imagepipeline</code>的继承类中个对下面的两个方法进行重载, <code>get_media_requests(item, info)</code>和<code>item_completed(results, items, info)</code></p>
<p>其中, 前者返回一个<code>Request</code>对象, 交给后者来处理, 后者处理之后会返回一个元组, 包含是否成功的标识以及图片信息或者报错信息.</p>
<p>接下来就来试试吧. 先把最基本的爬虫写了:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> photo_crawler.items <span class="keyword">import</span> PictureItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PicSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'Douban_crawler'</span></span><br><span class="line">    start_urls = [<span class="string">'https://www.douban.com/photos/album/1655441250/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        pic_list = response.xpath(<span class="string">'//div[@class="photolst clearfix"]/div/a/@href'</span>).extract()</span><br><span class="line">        print(pic_list)</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> pic_list:</span><br><span class="line">            page = page + <span class="string">"large/"</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=page, callback=self.parse_large)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_large</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        picture = PictureItem()</span><br><span class="line">        ori_pic_url = response.xpath(<span class="string">'//*[@id="pic-viewer"]/a/img/@src'</span>).extract()</span><br><span class="line">        picture[<span class="string">'image_urls'</span>] = ori_pic_url</span><br><span class="line">        <span class="keyword">yield</span> picture</span><br></pre></td></tr></table></figure>

<p>这里我们的示例来自豆瓣的相册.</p>
<p>上面的内容大体在干什么基本都是我上面说过的, 主要是来看看pipeline做了什么:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PhotoCrawlerPipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"cookie"</span>:<span class="string">"bid=MsOyVJ-WczQ"</span>,</span><br><span class="line">        <span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6)  \</span></span><br><span class="line"><span class="string">        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(item[<span class="string">'image_urls'</span>][<span class="number">0</span>], headers=self.headers, meta=item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        print(results)</span><br><span class="line">        path = [img[<span class="string">'path'</span>] <span class="keyword">for</span> ok, img <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> path:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">'No path'</span>)</span><br><span class="line">        item[<span class="string">'image_paths'</span>] = path</span><br></pre></td></tr></table></figure>

<p>这里我们简单的写了一个头部, 然后就重载了上面说的那两个方法.</p>
<p>这样执行之后, 我们就可以看到在<code>SETTING.py</code>文件中定义的存储路径下的图片了, 但是这个时候图片都是SHA-1算法的摘要值, 显然不方便辨认, 所以这就需要我们进行重新命名, 可以在pipeline中加上这个方法:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file_path</span><span class="params">(self, request, response=None, info=None)</span>:</span></span><br><span class="line">    ext = request.url.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">-1</span>]</span><br><span class="line">    item_name = request.meta[<span class="string">'images'</span>][<span class="number">0</span>]</span><br><span class="line">    path = <span class="string">'full/&#123;&#125;.&#123;&#125;'</span>.format(item_name, ext)</span><br><span class="line">    print(path)</span><br><span class="line">    <span class="keyword">return</span> path</span><br></pre></td></tr></table></figure>

<p>返回的路径就是文件的位置, 当然也包括了他自己的文件名和后缀. 而且在这里使用到了上面的<code>get_media_requests</code>中meta参数传递的item.</p>
<h1 id="并发编程-多并发爬虫"><a href="#并发编程-多并发爬虫" class="headerlink" title="并发编程, 多并发爬虫"></a>并发编程, 多并发爬虫</h1><p>在前面的Scrapy使用中 我们发现他爬取的速度非常快, 这是因为它使用了并发和异步的爬取模式.</p>
<p>我曾经在一开始学习Python的时候稍微了解过一些关于多线程, 多进程的并发编程. 但现在看来, 那个时候理解的显然不够深入, 因此我们现在再来把Python的并发编程复习一遍吧.</p>
<p>先来写几个小例子吧, 就当是复习了:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fork only work on Unix/Linux</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">pid = os.fork()</span><br><span class="line">print(<span class="string">"Forking..."</span>)</span><br><span class="line"><span class="keyword">if</span> pid == <span class="number">0</span>:</span><br><span class="line">    print(<span class="string">"This is child process %s"</span> % os.getpid())</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"parent process %s"</span> % os.getpid())</span><br></pre></td></tr></table></figure>

<p>但是我们知道这个fork仅仅工作在Unix/Linux平台上, 如果是为了更好的可移植性, 就应该使用一个Python为我们封装好的多进程库<code>multiprocessing</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_process</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"Running. (%s)"</span> % os.getpid())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    print(<span class="string">"parent process %s"</span> % os.getpid())</span><br><span class="line">    p = multiprocessing.Process(target=run_process)</span><br><span class="line">    p.start()</span><br><span class="line">    p.join()</span><br><span class="line">    print(<span class="string">"End"</span>)</span><br></pre></td></tr></table></figure>

<p>对于多线程的支持, Python提供连个模块: <code>_thread</code>和<code>threading</code>. 推荐使用后者, 因为其封装的更高层, 这样我就可以更加专注于业务逻辑的编写上.</p>
<p>说道多线程就一定会提到一个问题, 那就是<strong>资源争用</strong>. 下面的实验可以的话尽量使用Python2来做测试.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time, threading</span><br><span class="line"></span><br><span class="line">balance = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_it</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> balance</span><br><span class="line">    balance = balance + n</span><br><span class="line">    balance = balance - n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_thread</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100000</span>):</span><br><span class="line">        change_it(n)</span><br><span class="line"></span><br><span class="line">t1 = threading.Thread(target=run_thread, args=(<span class="number">5</span>,))</span><br><span class="line">t2 = threading.Thread(target=run_thread, args=(<span class="number">8</span>,))</span><br><span class="line">t1.start()</span><br><span class="line">t2.start()</span><br><span class="line">t1.join()</span><br><span class="line">t2.join()</span><br><span class="line">print(balance)</span><br></pre></td></tr></table></figure>

<p>执行之后就会发现balance的值发生了变化.</p>
<p>那么怎么办呢, 加个锁就行了:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time, threading</span><br><span class="line"></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_thread</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100000</span>):</span><br><span class="line">        lock.acquire()</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            change_it(n)</span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            lock.release()</span><br></pre></td></tr></table></figure>

<p>速度测试:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  time python test.py</span><br><span class="line">52</span><br><span class="line">python test.py  0.06s user 0.04s system 117% cpu 0.081 total</span><br><span class="line">➜  time python test.py</span><br><span class="line">50</span><br><span class="line">python test.py  0.06s user 0.04s system 115% cpu 0.081 total</span><br><span class="line">➜  time python test.py</span><br><span class="line">10</span><br><span class="line">python test.py  0.06s user 0.04s system 117% cpu 0.085 total</span><br></pre></td></tr></table></figure>

<p>在加了锁之后:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  time python test.py</span><br><span class="line">0</span><br><span class="line">python test.py  0.16s user 0.16s system 133% cpu 0.237 total</span><br><span class="line">➜  time python test.py</span><br><span class="line">0</span><br><span class="line">python test.py  0.16s user 0.15s system 130% cpu 0.234 total</span><br><span class="line">➜  time python test.py</span><br><span class="line">0</span><br><span class="line">python test.py  0.16s user 0.16s system 131% cpu 0.245 total</span><br></pre></td></tr></table></figure>

<p>接着我们再说一下线程安全的(当然也有对多进程的)队列, Python提供了先进先出的, 后进先出的, 以及优先队列三种. 当队列中没有元素的时候获取就会发生阻塞, 同样, 当队列满了的时候加入元素也会造成阻塞.</p>
<p>一个消费者/生产者模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">q = queue.Queue()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(*args)</span>:</span></span><br><span class="line">        print(<span class="string">"Current thread: %s started."</span> % threading.current_thread().name)</span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@debug</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        time.sleep(randint(<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line">        q.put(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@debug</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pull</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        q.get()</span><br><span class="line">        print(<span class="string">"Get one element. Current size: %d"</span> % q.qsize())</span><br><span class="line"></span><br><span class="line">threads = [ threading.Thread(target=push, name=<span class="string">"Push_thread"</span>), threading.Thread(target=pull, name=<span class="string">"Pull_thread"</span>)]</span><br><span class="line"><span class="keyword">for</span> thread <span class="keyword">in</span> threads:</span><br><span class="line">    thread.start()</span><br></pre></td></tr></table></figure>

<p>样例输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  python3 queueTest.py</span><br><span class="line">Current thread: Push_thread started.</span><br><span class="line">Current thread: Pull_thread started.</span><br><span class="line">Get one element. Current size: 0</span><br><span class="line">Get one element. Current size: 0</span><br><span class="line">Get one element. Current size: 3</span><br><span class="line">Get one element. Current size: 2</span><br><span class="line">Get one element. Current size: 1</span><br><span class="line">Get one element. Current size: 0</span><br></pre></td></tr></table></figure>

<p>我们在之前也提到过一个叫threadLocal的玩意, 这个东西其实你可以理解成是为了方便我们的变量获取而封装的一个东西.</p>
<p>由于我们创建和销毁进程是需要成本的, 所以使用<strong>线程池</strong>就可以充分利用CPU以及降低切换成本. 例子就略啦.</p>
<p>接下来就来开始这一部分的正式内容吧, 先来说说协程. 使用select进行一步IO, 缺点在于不能实现消息循环和状态控制.</p>
<p>来一段代码来做演示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">wget</span><span class="params">(host)</span>:</span></span><br><span class="line">    print(<span class="string">"wget %s..."</span> % host)</span><br><span class="line">    connect = asyncio.open_connection(host, <span class="number">80</span>)</span><br><span class="line">    reader, writer = <span class="keyword">await</span> connect</span><br><span class="line">    header = <span class="string">'GET / HTTP/1.1\r\nHost: %s\r\n\r\n'</span> % host</span><br><span class="line">    writer.write(header.encode(<span class="string">'utf-8'</span>))</span><br><span class="line">    <span class="keyword">await</span> writer.drain()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        line = <span class="keyword">await</span> reader.readline()</span><br><span class="line">        <span class="keyword">if</span> line == <span class="string">b'\r\n'</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        print(<span class="string">'%s header &gt; %s'</span> % (host, line.decode(<span class="string">'utf-8'</span>)))</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">hosts = [<span class="string">'www.google.com'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'www.sina.com'</span>, <span class="string">'www.douban.com'</span>]</span><br><span class="line">tasks = [wget(host) <span class="keyword">for</span> host <span class="keyword">in</span> hosts]</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">loop.close()</span><br></pre></td></tr></table></figure>

<p>这里的<code>async</code>和<code>await</code>是Python3.5以后的版本中新增的关键字, 如果是以前的版本, 应该使用<code>@asyncio.coroutine</code>装饰器和<code>yield from</code>关键字.</p>
<p>尽管google没法访问到, 但是我们依然可以不受到干扰的得到其他的头部信息.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#! /usr/bin/env python3</span></span><br><span class="line"><span class="comment">#######################################################################</span></span><br><span class="line"><span class="comment"># File Name: pic_crawler.py</span></span><br><span class="line"><span class="comment"># Author:Justin</span></span><br><span class="line"><span class="comment"># mail:justin13wyx@gmail.com</span></span><br><span class="line"><span class="comment"># Created Time: Tue Dec 12 20:53:47 2017</span></span><br><span class="line"><span class="comment"># ==============================================================</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> path</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(url, name=None, path=<span class="string">"/tmp/"</span>)</span>:</span></span><br><span class="line">    data = requests.get(url).content</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> name:</span><br><span class="line">        name = url.split(<span class="string">"/"</span>)[<span class="number">-1</span>]</span><br><span class="line">    path = path.join([name])</span><br><span class="line">    print(<span class="string">"downloading %s"</span> % path)</span><br><span class="line">    <span class="keyword">with</span> open(path, <span class="string">'wb'</span>) <span class="keyword">as</span> pic:</span><br><span class="line">        pic.write(data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(content)</span>:</span></span><br><span class="line">    data = json.loads(content, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">    imgs = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data[<span class="string">"list"</span>]:</span><br><span class="line">        sub_item = item[<span class="string">"arr"</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="string">"image"</span> <span class="keyword">in</span> sub_item[<span class="string">"type"</span>]:</span><br><span class="line">            imgs.extend(<span class="string">"http://litten.me/ins/%s.jpg"</span> % img <span class="keyword">for</span> img <span class="keyword">in</span> sub_item[<span class="string">"link"</span>])</span><br><span class="line">    <span class="keyword">for</span> thread <span class="keyword">in</span> [ threading.Thread(target=download, name=<span class="string">"download_thread"</span>, args=(img,)) <span class="keyword">for</span> img <span class="keyword">in</span> imgs ]:</span><br><span class="line">        thread.start()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(url=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> url:</span><br><span class="line">        stamp = int(datetime.now().timestamp() * <span class="number">1000</span>)</span><br><span class="line">        target = <span class="string">"http://litten.me/photos/ins.json?t=%s"</span> % str(stamp)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        target = url</span><br><span class="line">    content = requests.get(target).text</span><br><span class="line">    parse(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    crawl()</span><br></pre></td></tr></table></figure>


  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Python对XML-JSON-HTML的解析"><span class="toc-number">1.</span> <span class="toc-text">Python对XML,JSON,HTML的解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#xml"><span class="toc-number">1.0.1.</span> <span class="toc-text">xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JSON"><span class="toc-number">1.0.2.</span> <span class="toc-text">JSON</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HTML"><span class="toc-number">1.0.3.</span> <span class="toc-text">HTML</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Request库的使用"><span class="toc-number">2.</span> <span class="toc-text">Request库的使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BeautifulSoup-4的使用"><span class="toc-number">3.</span> <span class="toc-text">BeautifulSoup 4的使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sqlite"><span class="toc-number">4.</span> <span class="toc-text">sqlite</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#土家购模拟登录"><span class="toc-number">5.</span> <span class="toc-text">土家购模拟登录</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bilibili排行榜和动态通知"><span class="toc-number">6.</span> <span class="toc-text">Bilibili排行榜和动态通知</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Python爬虫框架-—-scrapy"><span class="toc-number">7.</span> <span class="toc-text">Python爬虫框架 — scrapy</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#并发编程-多并发爬虫"><span class="toc-number">8.</span> <span class="toc-text">并发编程, 多并发爬虫</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&text=Python爬虫学习笔记"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&is_video=false&description=Python爬虫学习笔记"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python爬虫学习笔记&body=Check out this article: http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&title=Python爬虫学习笔记"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yaoxuannn.com/2017/11/04/Python爬虫学习笔记/&name=Python爬虫学习笔记&description=&lt;p&gt;今天开始学习写爬虫吧~&lt;/p&gt;"><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 Yaoxuan Wei
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


</body>
</html>
